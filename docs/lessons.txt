16.11.25

For reduced dataset,
1. For small values of K, standard KNN worked well.

2. But for SVM-KNN, small values of K showed poor results.
   With greater K values, when there was a diagreement
   amongst the votes (i.e. unanimous voting failed), the
   SVM was able to catch mistakes that the standard KNN
   would have made anyway. For instance, if the true label
   was supposed to be 8 but the votes said that 4 was most
   likely, SVM outputted 8 nonetheless.


16.11.26

On USPS dataset,
1. Normalizing values with range [-1,1] to [0,1] did not
   make a difference.


16.11.27

On USPS dataset,
1. SVM KNN performance improves then does bad.
    Reason: k = 30, 80 not suitable because n=7000.
    --> fetches data points that are maybe too far away.
    --> Paper uses K=10 in fact.

    ==> Maybe we should try 3 <= K <= 10 more values in 
        this range?

2. The reason we use this hybrid is to improve accuracy.
   We see that at certain K, possibily proportionate to 
   the number of samples, SVM KNN may achieve an the best
   accuracy better than that of KNN. (Pyramid because for
   too small or too large values of K, accuracy drops.)

   Take MNIST: about 60,000 data points, K = 80
   Take USPS : about  7,000 data points, K = 10
    --> approimately proportionate!!!

